{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUagcDxddOS0"
      },
      "outputs": [],
      "source": [
        "# iii = ['iii']*999\n",
        "# while True:\n",
        "#   iii.append(iii)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivj8CV7U8fsV",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhUDEHew64yL",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!unzip 'drive/MyDrive/data.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAAhe5GX5-W8",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install pydicom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfN3jjj92Qkp",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install xlsxwriter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'"
      ],
      "metadata": {
        "id": "Oxt59_4-K7ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uq3Hivm667dS"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import pydicom\n",
        "import xlsxwriter\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import matplotlib.image as mpimg\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, LambdaCallback\n",
        "from math import sqrt\n",
        "from os import walk\n",
        "from random import random, randint\n",
        "from copy import deepcopy\n",
        "from scipy.ndimage import rotate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VaVOREuby4Z"
      },
      "outputs": [],
      "source": [
        "CASE_SE = 'Without-scSE'\n",
        "RES_FOLDER = f'drive/MyDrive/SRN-results/{CASE_SE}'\n",
        "\n",
        "# Створюємо папки, якщо їх немає\n",
        "os.makedirs(RES_FOLDER, exist_ok=True)\n",
        "os.makedirs(f'{RES_FOLDER}/imgs', exist_ok=True)\n",
        "\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 0.00005\n",
        "\n",
        "HP = {\n",
        "    'CASE_SE': CASE_SE,\n",
        "    'NUM_EPOCHS': NUM_EPOCHS,\n",
        "    'BATCH_SIZE': BATCH_SIZE,\n",
        "    'LEARNING_RATE': LEARNING_RATE,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def resize(arr, new_size):\n",
        "  return cv2.resize(arr, dsize=new_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "\n",
        "def normalize(dataset, max_val):\n",
        "  dataset[:, 0] /= max_val\n",
        "  dataset[:, 1][dataset[:, 1] > 0] = 1\n",
        "\n",
        "\n",
        "def get_mask(filename):\n",
        "  mask = []\n",
        "  with open(filename, 'rb') as f:  # Читаем файл в бинарном режиме\n",
        "    for l in f:  # Читаем построчно (по факту всего одна строка)\n",
        "      sqrt_l = int(sqrt(len(l)))  # Получаем высоту и ширину маски (например, 1 строка 65536 символов - это квадратная маска 256х256=65536)\n",
        "      # При этом полученные размеры маски вдвое больше размеров изображения (например, маска 256х256 для изображения 128х128)\n",
        "      # Берем каждые sqrt_l*2 значений маски из строки и записываем в двумерный массив маски\n",
        "      for i in range(int(sqrt_l/4)):\n",
        "        for j in range(2):\n",
        "          mask.append([int(s) for s in l[i*sqrt_l*4+j*sqrt_l*2:i*sqrt_l*4+j*sqrt_l*2+sqrt_l*2]])  # Одновременно переводим байти в int\n",
        "      # В результате получаем маску: sqrt_l/2(высота) х sqrt_l*2(ширина)\n",
        "      # Нужная высота уже есть (=sqrt_l/2), осталось получить нужную ширину (=sqrt_l/2)\n",
        "  mask = np.array(mask)\n",
        "  mask = np.delete(mask, range(0, sqrt_l*2, 2), 1)  # Удаляем каждый второй столбец маски с лишними нулями\n",
        "  mask = np.delete(mask, range(0, sqrt_l, 2), 1)  # Удаляем каждый второй столбец маски с лишними нулями и значениями\n",
        "  # *Это я дополнительно приводил все маски к одному размеру для обучения нейронки\n",
        "  if mask.shape != (128, 128):\n",
        "    mask = resize(mask, (128, 128))\n",
        "  return mask\n",
        "\n",
        "\n",
        "def getDataset(dirname):\n",
        "\n",
        "  def get_set_name(dataset, curr_ds):\n",
        "    new_len = sum(len(item) for item in dataset['data'].values()) + len(curr_ds)\n",
        "    ideal = {'train': dataset['size']['train']*new_len,\n",
        "             'validation': dataset['size']['validation']*new_len,\n",
        "             'test': dataset['size']['test']*new_len}\n",
        "    errors = {'train': (abs(len(dataset['data']['train']) + len(curr_ds) - ideal['train']) +\n",
        "                        abs(len(dataset['data']['validation']) - ideal['validation']) +\n",
        "                        abs(len(dataset['data']['test']) - ideal['test'])),\n",
        "              'validation': (abs(len(dataset['data']['train']) - ideal['train']) +\n",
        "                        abs(len(dataset['data']['validation']) + len(curr_ds) - ideal['validation']) +\n",
        "                        abs(len(dataset['data']['test']) - ideal['test'])),\n",
        "              'test': (abs(len(dataset['data']['train']) - ideal['train']) +\n",
        "                        abs(len(dataset['data']['validation']) - ideal['validation']) +\n",
        "                        abs(len(dataset['data']['test']) + len(curr_ds) - ideal['test']))}\n",
        "    return min(errors, key=errors.get)\n",
        "\n",
        "  dataset = {'data': {'train': [], 'validation': [], 'test': []},\n",
        "            #  'size': {'train': 0.64, 'validation': 0.16, 'test': 0.2},\n",
        "            #  'size': {'train': 0.72, 'validation': 0.13, 'test': 0.15},\n",
        "             'size': {'train': 0.68, 'validation': 0.12, 'test': 0.2},\n",
        "             'len': {'train': 0, 'validation': 0, 'test': 0},\n",
        "             'file': {'train': [], 'validation': [], 'test': []}}\n",
        "  for f in walk(dirname):  # Рекурсивно проходимся по всем папкам датасета\n",
        "    if len(f[2]) > 0:  # Если папка содержит файлы\n",
        "      dirpath = f[0]  # Записываем в переменную путь к файлам\n",
        "      dirname = dirpath.split('/')[1]\n",
        "      filenames = sorted(f[2])  # Сортируем имена файлов\n",
        "\n",
        "      masknames = [fn for fn in filenames if '.txt' in fn]  # Записываем в переменную отсортированные имена масок в папке\n",
        "      masks = {}\n",
        "      if len(masknames) > 0:  # Если папка содержит маски\n",
        "        for mn in masknames:  # Перебираем циклом имена масок\n",
        "          idx = int(mn[:-4].split('-')[1])  # Получаем индекс маски из ее названия\n",
        "          masks[idx] = get_mask(f'{dirpath}/{mn}')  # Добавляем маску в словарь\n",
        "      else:\n",
        "        continue\n",
        "\n",
        "      dcm_groups = {}\n",
        "      dcm_filenames = {}\n",
        "      for fn in filenames:  # Перебираем циклом имена файлов\n",
        "        if '.dcm' in fn:  # Отбираем только dcm-файлы\n",
        "          dcm_file = pydicom.dcmread(f'{dirpath}/{fn}')  # Считываем и записываем dcm-файл в переменную\n",
        "          if dcm_file.SliceLocation in dcm_groups.keys():  # Если словарь dcm_groups содержит ключь = значению тега SliceLocation данного dcm-файла\n",
        "            dcm_groups[dcm_file.SliceLocation][dcm_file.InstanceNumber] = dcm_file\n",
        "            dcm_filenames[dcm_file.SliceLocation][dcm_file.InstanceNumber] = fn\n",
        "          else:\n",
        "            dcm_groups[dcm_file.SliceLocation] = {dcm_file.InstanceNumber: dcm_file}  # Добавляем в словарь по ключу SliceLocation массив с dcm-файлом\n",
        "            dcm_filenames[dcm_file.SliceLocation] = {dcm_file.InstanceNumber: fn}\n",
        "\n",
        "      sorted_dcm_groups = sorted(dcm_groups)  # Сортируем ключи (SliceLocation) словаря dcm_groups\n",
        "      sorted_dcm_filenames = sorted(dcm_filenames)\n",
        "\n",
        "      curr_ds = []\n",
        "      curr_len = 0\n",
        "      cur_filenames = []\n",
        "      for k in sorted_dcm_groups:  # Перебираем отсортированные ключи (SliceLocation)\n",
        "        if sorted_dcm_groups.index(k) in masks.keys():  # Если к группе файлов есть маска\n",
        "          curr_mask = masks[sorted_dcm_groups.index(k)]  # Записывам в переменную текущую маску\n",
        "          flatten_mask = curr_mask.flatten()\n",
        "          if flatten_mask.shape[0]*0.1 < np.count_nonzero(flatten_mask != flatten_mask[0]) < flatten_mask.shape[0]*0.9:  # Если нулевой элемент массива маски повторяется больше 10%, но меньше 90% (а то бывает, что выделено 2 пикселя и это типо маска - что явно не верно)\n",
        "            k2 = sorted(dcm_groups[k])[3]\n",
        "            curr_img = dcm_groups[k][k2].pixel_array  # Берем массив пикселей dcm-файла\n",
        "            if curr_img.shape != (128, 128):  # Если размер не 128х128\n",
        "              curr_img = resize(curr_img, (128, 128))  # Приводим к размеру 128х128\n",
        "            curr_ds.insert(0, [curr_img, curr_mask])\n",
        "            curr_len += 1\n",
        "            cur_filenames.insert(0, f'{dirname}/{dcm_filenames[k][k2]}')\n",
        "\n",
        "      set_name = get_set_name(dataset, curr_ds)\n",
        "      dataset['data'][set_name] = curr_ds[:curr_len] + dataset['data'][set_name] + curr_ds[curr_len:]\n",
        "      dataset['len'][set_name] += curr_len\n",
        "      dataset['file'][set_name] = cur_filenames[:curr_len] + dataset['file'][set_name] + cur_filenames[curr_len:]\n",
        "\n",
        "  for key in dataset['data'].keys():\n",
        "    dataset['data'][key] = np.array(dataset['data'][key], dtype=float)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "\n",
        "dataset = getDataset('TCGA-GBM-raw')\n",
        "max_val = np.max(np.concatenate((dataset['data']['train'][:, 0],\n",
        "                                 dataset['data']['validation'][:, 0],\n",
        "                                 dataset['data']['test'][:, 0]), axis=0))\n",
        "for key in dataset['data'].keys():\n",
        "  normalize(dataset['data'][key], max_val)\n",
        "  dataset['data'][key] = dataset['data'][key].reshape((*dataset['data'][key].shape, 1))\n",
        "  print(dataset['data'][key].shape, dataset['len'][key], key, sep='\\t')"
      ],
      "metadata": {
        "id": "YSsqKH3Vd8Wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqg5MxICBXa0"
      },
      "outputs": [],
      "source": [
        "class CustomDataGen(Sequence):\n",
        "\n",
        "    def __init__(self, dataset,\n",
        "                 batch_size,\n",
        "                 input_size=(128, 128, 1),\n",
        "                 do_augment=False\n",
        "                 ):\n",
        "      self.ds = deepcopy(dataset)\n",
        "      self.batch_size = batch_size\n",
        "      self.input_size = input_size\n",
        "      self.n = self.ds.shape[0]\n",
        "\n",
        "      self.do_augment = do_augment;\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "      np.random.shuffle(self.ds)\n",
        "\n",
        "    def augment(self, batches):\n",
        "      for i in range(batches.shape[0]):\n",
        "        if random() > 0.5:\n",
        "          batches[i, 0] = np.fliplr(batches[i, 0])\n",
        "          batches[i, 1] = np.fliplr(batches[i, 1])\n",
        "        if random() > 0.5:\n",
        "          max_0 = np.max(batches[i, 0])\n",
        "          angle = randint(-15, 15)\n",
        "          batches[i, 0] = rotate(batches[i, 0], angle, reshape=False)\n",
        "          batches[i, 1] = rotate(batches[i, 1], angle, reshape=False)\n",
        "          # normalization\n",
        "          batches[i, 0] = ((batches[i, 0] - np.min(batches[i, 0])) / (np.max(batches[i, 0]) - np.min(batches[i, 0])))*max_0\n",
        "          batches[i, 1] = np.around((batches[i, 1] - np.min(batches[i, 1])) / (np.max(batches[i, 1]) - np.min(batches[i, 1])))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      batches = deepcopy(self.ds[index*self.batch_size:(index + 1)*self.batch_size])\n",
        "      if self.do_augment:\n",
        "        self.augment(batches)  # аугментация\n",
        "      X = batches[:, 0]\n",
        "      y = batches[:, 1]\n",
        "      return X, y\n",
        "\n",
        "    def __len__(self):\n",
        "      return int(self.n // self.batch_size)\n",
        "\n",
        "\n",
        "train_gen = CustomDataGen(dataset['data']['train'], BATCH_SIZE, do_augment=True)\n",
        "validation_gen = CustomDataGen(dataset['data']['validation'], BATCH_SIZE)\n",
        "test_gen = CustomDataGen(dataset['data']['test'], dataset['len']['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_krG4AW8IJT"
      },
      "outputs": [],
      "source": [
        "def sSE_block(x):\n",
        "\n",
        "  x_1 = x\n",
        "  x = layers.Conv2D(1, 1, activation=\"sigmoid\")(x)\n",
        "  x = layers.multiply([x, x_1])\n",
        "\n",
        "  return x\n",
        "\n",
        "\n",
        "def cSE_block(x, n_filters):\n",
        "\n",
        "  x_1 = x\n",
        "  x = layers.GlobalAveragePooling2D(keepdims=True)(x)\n",
        "  x = layers.Conv2D(n_filters/2, 1, activation=\"relu\")(x)\n",
        "  x = layers.Conv2D(n_filters, 1, activation=\"sigmoid\")(x)\n",
        "  x = layers.multiply([x, x_1])\n",
        "\n",
        "  return x\n",
        "\n",
        "\n",
        "def scSE_block(x, n_filters):\n",
        "\n",
        "  sSE = sSE_block(x)\n",
        "  cSE = cSE_block(x, n_filters)\n",
        "  x = layers.add([sSE, cSE])\n",
        "\n",
        "  return x\n",
        "\n",
        "\n",
        "def downsample_block(x, n_filters):\n",
        "\n",
        "    x_1 = layers.Conv2D(1, 1, activation=\"relu\")(x)\n",
        "    x_1 = layers.MaxPool2D(2)(x_1)\n",
        "    x = layers.Conv2D(n_filters, 3, 2, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.Conv2D(n_filters, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.add([x, x_1])\n",
        "\n",
        "    x_2 = scSE_block(x, n_filters) if CASE_SE == 'scSE-Identity' else x  # scSE-Identity\n",
        "    x = scSE_block(x, n_filters) if CASE_SE == 'scSE-PRE' else x  # scSE-PRE\n",
        "    x = layers.Conv2D(n_filters, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.Conv2D(n_filters, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = scSE_block(x, n_filters) if CASE_SE == 'Standard-scSE' else x  # Standard-scSE\n",
        "    x = layers.add([x, x_2])\n",
        "    x = scSE_block(x, n_filters) if CASE_SE == 'scSE-POST' else x  # scSE-POST\n",
        "\n",
        "    f = x\n",
        "    p = x\n",
        "\n",
        "    return f, p\n",
        "\n",
        "\n",
        "def upsample_block(x, conv_features, n_filters):\n",
        "\n",
        "    x = layers.Conv2DTranspose(n_filters, 3, 2, padding=\"same\")(x)\n",
        "    x = layers.concatenate([x, conv_features])\n",
        "\n",
        "    x_1 = layers.Conv2D(1, 1, activation=\"relu\")(x)\n",
        "    x = layers.Conv2D(n_filters, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.Conv2D(n_filters, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.add([x, x_1])\n",
        "\n",
        "    x_2 = scSE_block(x, n_filters) if CASE_SE == 'scSE-Identity' else x  # scSE-Identity\n",
        "    x = scSE_block(x, n_filters) if CASE_SE == 'scSE-PRE' else x  # scSE-PRE\n",
        "    x = layers.Conv2D(n_filters, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.Conv2D(n_filters, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = scSE_block(x, n_filters) if CASE_SE == 'Standard-scSE' else x  # Standard-scSE\n",
        "    x = layers.add([x, x_2])\n",
        "    x = scSE_block(x, n_filters) if CASE_SE == 'scSE-POST' else x  # scSE-POST\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def build_model():\n",
        "\n",
        "    # inputs\n",
        "    inputs = layers.Input(shape=(128, 128, 1))\n",
        "\n",
        "    f0 = layers.Conv2D(64, 7, 2, padding='same', activation=\"relu\")(inputs)\n",
        "    p0 = f0\n",
        "\n",
        "    # encoder: contracting path - downsample\n",
        "    f1, p1 = downsample_block(p0, 64)\n",
        "    f2, p2 = downsample_block(p1, 128)\n",
        "    f3, p3 = downsample_block(p2, 256)\n",
        "    f4, p4 = downsample_block(p3, 512)\n",
        "\n",
        "    # decoder: expanding path - upsample\n",
        "    u5 = upsample_block(p4, f3, 512)\n",
        "    u6 = upsample_block(u5, f2, 256)\n",
        "    u7 = upsample_block(u6, f1, 128)\n",
        "\n",
        "    u8 = layers.Conv2DTranspose(64, 3, 2, padding=\"same\")(u7)\n",
        "    u8 = layers.concatenate([u8, f0])\n",
        "    u8 = layers.Conv2DTranspose(64, 7, 2, padding=\"same\")(u8)\n",
        "\n",
        "    # outputs\n",
        "    outputs = layers.Conv2D(2, 1, padding=\"same\", activation=\"softmax\")(u8)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oueZUwyJ8S5a"
      },
      "outputs": [],
      "source": [
        "scc = SparseCategoricalCrossentropy()\n",
        "val_loss_begin = []\n",
        "val_loss_begin_clbk = LambdaCallback(on_epoch_begin=lambda epoch, logs:\n",
        "                                     val_loss_begin.append(scc(dataset['data']['validation'][:, 1],\n",
        "                                                               model.predict(dataset['data']['validation'][:, 0])).numpy()))\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
        "              loss=scc,\n",
        "              metrics=['sparse_categorical_accuracy'],\n",
        "              )\n",
        "\n",
        "model_history = model.fit(\n",
        "    x=train_gen,\n",
        "    validation_data=validation_gen,\n",
        "    epochs=NUM_EPOCHS,\n",
        "    callbacks=[val_loss_begin_clbk,\n",
        "               ReduceLROnPlateau(factor=0.1, patience=10),\n",
        "               ModelCheckpoint(\n",
        "                   filepath=f'{RES_FOLDER}/best_model.keras',\n",
        "                   save_best_only=True)\n",
        "               ])\n",
        "\n",
        "model = tf.keras.models.load_model(f'{RES_FOLDER}/best_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lB1vghXpvpKP",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyoiDOsZtwx1"
      },
      "outputs": [],
      "source": [
        "def get_stats(true_mask, pred_mask):\n",
        "  tp, tn, fp, fn = 0, 0, 0, 0\n",
        "  for i in range(pred_mask.shape[0]):\n",
        "    for j in range(pred_mask.shape[1]):\n",
        "      if pred_mask[i, j] == true_mask[i, j]:\n",
        "        if pred_mask[i, j] == 0:\n",
        "          tn += 1\n",
        "        else:\n",
        "          tp += 1\n",
        "      else:\n",
        "        if pred_mask[i, j] == 0:\n",
        "          fn += 1\n",
        "        else:\n",
        "          fp += 1\n",
        "  return tp, tn, fp, fn\n",
        "\n",
        "\n",
        "def get_dice(tp, tn, fp, fn):\n",
        "  return 2*tp / (2*tp + fp + fn)\n",
        "\n",
        "\n",
        "def get_sens(tp, tn, fp, fn):\n",
        "  return tp / (tp + fn)\n",
        "\n",
        "\n",
        "def get_spec(tp, tn, fp, fn):\n",
        "  return tn / (tn + fp)\n",
        "\n",
        "\n",
        "def get_accu(tp, tn, fp, fn):\n",
        "  return (tp + tn) / (tp + tn + fp + fn)\n",
        "\n",
        "\n",
        "def get_metrics(tp, tn, fp, fn):\n",
        "  dice = get_dice(tp, tn, fp, fn)\n",
        "  sens = get_sens(tp, tn, fp, fn)\n",
        "  spec = get_spec(tp, tn, fp, fn)\n",
        "  accu = get_accu(tp, tn, fp, fn)\n",
        "  return dice, sens, spec, accu\n",
        "\n",
        "\n",
        "def create_mask(pred_mask):\n",
        "  pred_mask = np.argmax(pred_mask, axis=-1)\n",
        "  pred_mask = pred_mask[..., np.newaxis]\n",
        "  return pred_mask[1]\n",
        "\n",
        "\n",
        "def true_pred_mask(true_mask, pred_mask):\n",
        "  true_pred_mask = deepcopy(pred_mask)\n",
        "  for i in range(pred_mask.shape[0]):\n",
        "    for j in range(pred_mask.shape[1]):\n",
        "      if true_mask[i, j] == 0:\n",
        "        true_pred_mask[i, j] = 2 if pred_mask[i, j] == 1 else 0\n",
        "      else:  # 1\n",
        "        true_pred_mask[i, j] = 3 if pred_mask[i, j] == 0 else 1\n",
        "  palette = np.array([[  0,   0,   0],   # tn - black\n",
        "                      [255,   0,   0],   # tp - red\n",
        "                      [  0, 255,   0],   # fp - green\n",
        "                      [  0,   0, 255]],  # fn - blue\n",
        "                      dtype='uint8')\n",
        "  return palette[true_pred_mask][..., np.newaxis]\n",
        "\n",
        "\n",
        "def show_predictions(dataset):\n",
        "  for d in dataset:\n",
        "    pred_mask = create_mask(model.predict(d, verbose=0))\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "    i = 0\n",
        "    titles = ['input image', 'true mask', 'pred mask', 'tn(0) tp(r) fp(g) fn(b)']\n",
        "    for img in [d[0], d[1], pred_mask, true_pred_mask(d[1], pred_mask)]:\n",
        "      fig.add_subplot(1, 4, i+1)\n",
        "      plt.imshow(np.squeeze(img))\n",
        "      plt.axis('off')\n",
        "      plt.title(titles[i])\n",
        "      i += 1\n",
        "    plt.show()\n",
        "    tp, tn, fp, fn = get_stats(d[1], pred_mask)\n",
        "    print(f'TP = {tp}, TN = {tn}, FP = {fp}, FN = {fn}')\n",
        "    print(f'DICE = {get_dice(tp, tn, fp, fn)}')\n",
        "    print(f'SENS = {get_sens(tp, tn, fp, fn)}')\n",
        "    print(f'SPEC = {get_spec(tp, tn, fp, fn)}')\n",
        "    print(f'ACCU = {get_accu(tp, tn, fp, fn)}\\n')\n",
        "\n",
        "\n",
        "show_predictions(dataset['data']['test'][:1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_results(dataset):\n",
        "\n",
        "  def add_channels(img, imgtype='mask'):\n",
        "    channels_img = np.zeros((img.shape[0], img.shape[1], 3), dtype='uint8')\n",
        "    if imgtype == 'mask':\n",
        "      channels_img[img[:, :, 0] == 1] = np.array([255, 0, 0], dtype='uint8')\n",
        "    elif imgtype == 'img':\n",
        "      img_max = np.amax(img)\n",
        "      for x in range(img.shape[0]):\n",
        "        for y in range(img.shape[1]):\n",
        "          for z in range(channels_img.shape[2]):\n",
        "            channels_img[x, y, z] = round(255*(img[x, y, 0] / img_max))\n",
        "    return channels_img\n",
        "\n",
        "  def save_accuracy():\n",
        "    plt.rcParams.update({'font.size': 14})\n",
        "    figure(figsize=(25, 10))\n",
        "    val_accuracy = model_history.history['val_sparse_categorical_accuracy']\n",
        "    max_val_accuracy = max(val_accuracy)\n",
        "    best_accuracy = plt.scatter(val_accuracy.index(max_val_accuracy)+1, max_val_accuracy, s=100, c='red', zorder=3)\n",
        "    t_accuracy = plt.plot(range(1, NUM_EPOCHS+1), model_history.history['sparse_categorical_accuracy'], 'b^-', zorder=1)\n",
        "    v_accuracy = plt.plot(range(1, NUM_EPOCHS+1), val_accuracy, 'gs-', zorder=2)\n",
        "    plt.xticks(range(0, NUM_EPOCHS+1, 5))\n",
        "    plt.title(CASE_SE)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.legend((t_accuracy[0], v_accuracy[0], best_accuracy),\n",
        "               ('train accuracy', 'validation accuracy', 'best model'),\n",
        "               loc='lower right')\n",
        "    plt.savefig(f'{RES_FOLDER}/accuracy.png')\n",
        "\n",
        "  def save_loss():\n",
        "    plt.rcParams.update({'font.size': 14})\n",
        "    figure(figsize=(25, 10))\n",
        "    val_loss = model_history.history['val_loss']\n",
        "    min_val_loss = min(val_loss)\n",
        "    best_loss = plt.scatter(val_loss.index(min_val_loss)+1, min_val_loss, s=100, c='red', zorder=3)\n",
        "    t_loss = plt.plot(range(1, NUM_EPOCHS+1), model_history.history['loss'], 'b^-', zorder=1)\n",
        "    v_loss = plt.plot(range(1, NUM_EPOCHS+1), val_loss, 'gs-', zorder=2)\n",
        "    plt.xticks(range(0, NUM_EPOCHS+1, 5))\n",
        "    plt.title(CASE_SE)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend((t_loss[0], v_loss[0], best_loss),\n",
        "               ('train loss', 'validation loss', 'best model'),\n",
        "               loc='upper right')\n",
        "    plt.savefig(f'{RES_FOLDER}/loss.png')\n",
        "\n",
        "  # val_loss_begin\n",
        "  def save_loss_begin():\n",
        "    plt.rcParams.update({'font.size': 14})\n",
        "    figure(figsize=(25, 10))\n",
        "    val_loss = val_loss_begin\n",
        "    min_val_loss = min(val_loss)\n",
        "    best_loss = plt.scatter(val_loss.index(min_val_loss)+1, min_val_loss, s=100, c='red', zorder=3)\n",
        "    t_loss = plt.plot(range(1, NUM_EPOCHS+1), model_history.history['loss'], 'b^-', zorder=1)\n",
        "    v_loss = plt.plot(range(1, NUM_EPOCHS+1), val_loss, 'gs-', zorder=2)\n",
        "    plt.xticks(range(0, NUM_EPOCHS+1, 5))\n",
        "    plt.title(CASE_SE)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend((t_loss[0], v_loss[0], best_loss),\n",
        "               ('train loss', 'validation loss', 'best model'),\n",
        "               loc='upper right')\n",
        "    plt.savefig(f'{RES_FOLDER}/loss_begin.png')\n",
        "\n",
        "  save_accuracy()\n",
        "  save_loss()\n",
        "  save_loss_begin()  # val_loss_begin\n",
        "  workbook = xlsxwriter.Workbook(f'{RES_FOLDER}/results.xlsx')\n",
        "  workbook.add_worksheet(CASE_SE)\n",
        "  worksheet = workbook.get_worksheet_by_name(CASE_SE)\n",
        "  sum_TP, sum_TN, sum_FP, sum_FN = 0, 0, 0, 0\n",
        "  row, col = 0, 0\n",
        "  for k, v in HP.items():\n",
        "    worksheet.write_row(row, col, [k, v])\n",
        "    row += 1\n",
        "  row += 1\n",
        "  worksheet.write_row(row, col, ['epoch', *range(1, NUM_EPOCHS+1)])\n",
        "  row += 1\n",
        "  for k, v in model_history.history.items():\n",
        "    worksheet.write_row(row, col, [k, *v])\n",
        "    row += 1\n",
        "  worksheet.write_row(row, col, ['val_loss_begin', *val_loss_begin])  # val_loss_begin\n",
        "  row += 2\n",
        "  worksheet.write_row(row, col, ['TP', 'TN', 'FP', 'FN', 'DICE', 'SENS', 'SPEC', 'ACCU', 'FILE'])\n",
        "  row += 1\n",
        "  img_idx = 0\n",
        "  dice_vals, sens_vals, spec_vals, accu_vals = np.array([]), np.array([]), np.array([]), np.array([])\n",
        "  for i in range(dataset['len']['test']):\n",
        "    d = dataset['data']['test'][i]\n",
        "    pred_mask = create_mask(model.predict(d, verbose=0))\n",
        "    # Save imgs\n",
        "    mpimg.imsave(f'{RES_FOLDER}/imgs/{img_idx}__input_image.png', add_channels(d[0], 'img'))\n",
        "    mpimg.imsave(f'{RES_FOLDER}/imgs/{img_idx}__true_mask.png', add_channels(d[1]))\n",
        "    mpimg.imsave(f'{RES_FOLDER}/imgs/{img_idx}__pred_mask.png', add_channels(pred_mask))\n",
        "    mpimg.imsave(f'{RES_FOLDER}/imgs/{img_idx}__tn(0)_tp(r)_fp(g)_fn(b).png',\n",
        "                 np.squeeze(true_pred_mask(d[1], pred_mask)))\n",
        "    img_idx += 1\n",
        "    tp, tn, fp, fn = get_stats(d[1], pred_mask)\n",
        "    dice, sens, spec, accu = get_metrics(tp, tn, fp, fn)\n",
        "    dice_vals = np.append(dice_vals, dice)\n",
        "    sens_vals = np.append(sens_vals, sens)\n",
        "    spec_vals = np.append(spec_vals, spec)\n",
        "    accu_vals = np.append(accu_vals, accu)\n",
        "    sum_TP += tp\n",
        "    sum_TN += tn\n",
        "    sum_FP += fp\n",
        "    sum_FN += fn\n",
        "    worksheet.write_row(row, col, [tp, tn, fp, fn, dice, sens, spec, accu, dataset['file']['test'][i]])\n",
        "    row += 1\n",
        "  row += 1\n",
        "  worksheet.write_row(row, col, ['', 'DICE', 'SENS', 'SPEC', 'ACCU'])\n",
        "  row += 1\n",
        "  worksheet.write_row(row, col, ['mean', np.mean(dice_vals), np.mean(sens_vals), np.mean(spec_vals), np.mean(accu_vals)])\n",
        "  row += 1\n",
        "  worksheet.write_row(row, col, ['std', np.std(dice_vals), np.std(sens_vals), np.std(spec_vals), np.std(accu_vals)])\n",
        "  row += 1\n",
        "  worksheet.write_row(row, col, ['min', np.min(dice_vals), np.min(sens_vals), np.min(spec_vals), np.min(accu_vals)])\n",
        "  row += 1\n",
        "  worksheet.write_row(row, col, ['max', np.max(dice_vals), np.max(sens_vals), np.max(spec_vals), np.max(accu_vals)])\n",
        "  row += 2\n",
        "  worksheet.write(row, col, 'TOTAL:')\n",
        "  row += 1\n",
        "  worksheet.write_row(row, col, ['TP', 'TN', 'FP', 'FN', 'DICE', 'SENS', 'SPEC', 'ACCU'])\n",
        "  row += 1\n",
        "  sum_DICE, sum_SENS, sum_SPEC, sum_ACCU = get_metrics(sum_TP, sum_TN, sum_FP, sum_FN)\n",
        "  worksheet.write_row(row, col, [sum_TP, sum_TN, sum_FP, sum_FN, sum_DICE, sum_SENS, sum_SPEC, sum_ACCU])\n",
        "  workbook.close()\n",
        "\n",
        "\n",
        "save_results(dataset)"
      ],
      "metadata": {
        "id": "Yzb-bmU0J4Jk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}